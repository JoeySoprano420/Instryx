
# Instryx Lexer â€” superior boosters, enhancers, tooling, optimizations
# Author: Violet Magenta / VACU Technologies (modified)
# License: MIT
"""
High-performance, fully-implemented lexer for the Instryx language.

Extra boosters & optimizations:
 - Robust token model (Token dataclass) with optional positional info and span
 - Fast precompiled combined regex with ordered, named patterns
 - String unescaping (via ast.literal_eval), numeric normalization (int/float, hex/bin, underscores)
 - Multi-line and single-line comment support; optional comment/whitespace skipping
 - Streaming generator API + convenience list API + simple backwards-compatible API
 - Optional per-instance LRU caching for repeated inputs
 - TokenStream helper with peek/next for parser convenience
 - Executable self-test in __main__
"""

import re
import bisect
import ast
from dataclasses import dataclass
from typing import List, Tuple, Iterator, Optional, Union, Iterable, Dict
from functools import lru_cache

# Public token shapes
TokenSimple = Tuple[str, str]
TokenWithPos = Tuple[str, Union[str, int, float], int, int]
Token = Union[TokenSimple, TokenWithPos]


@dataclass(frozen=True)
class TokenObj:
    type: str
    value: Union[str, int, float]
    lineno: Optional[int] = None
    col: Optional[int] = None
    start: Optional[int] = None
    end: Optional[int] = None

    def as_tuple(self, emit_pos: bool) -> Token:
        if emit_pos and self.lineno is not None and self.col is not None:
            return (self.type, self.value, self.lineno, self.col)
        return (self.type, self.value)


@dataclass
class LexerConfig:
    emit_positions: bool = False        # default False for backward compatibility
    skip_comments: bool = True
    skip_whitespace: bool = True
    enable_cache: bool = False
    cache_size: int = 128


class InstryxLexer:
    def __init__(self, config: Optional[LexerConfig] = None):
        self.config = config or LexerConfig()

        # Keyword set (overrideable)
        self.keywords = {
            'func', 'main', 'quarantine', 'try', 'replace', 'erase',
            'if', 'else', 'while', 'fork', 'join', 'then', 'true', 'false',
            'print', 'alert', 'log', 'return', 'import',
        }

        # Token specification (order matters: longest/multi-char first)
        specs = [
            ('ML_COMMENT', r'/\*[\s\S]*?\*/'),
            ('COMMENT',    r'--[^\n]*'),
            ('MACRO',      r'@[A-Za-z_][A-Za-z0-9_]*(?:\.[A-Za-z_][A-Za-z0-9_]*)*'),
            ('STRING',     r'"(?:\\.|[^"\\])*"'),
            ('NUMBER',     r'0x[0-9A-Fa-f_]+|0b[01_]+|\d+(?:_\d+)*(?:\.\d+(?:_\d+)*)?'),
            ('ASSIGN',     r'='),
            ('END',        r';'),
            ('DOT',        r'\.'),
            ('COLON',      r':'),
            ('COMMA',      r','),
            ('LPAREN',     r'\('),
            ('RPAREN',     r'\)'),
            ('LBRACE',     r'\{'),
            ('RBRACE',     r'\}'),
            ('OP',         r'==|!=|<=|>=|\|\||&&|\b(?:and|or|not)\b|[+\-*/%<>!]'),
            ('ID',         r'[A-Za-z_][A-Za-z0-9_]*'),
            ('NEWLINE',    r'\n'),
            ('SKIP',       r'[ \t\r]+'),
            ('MISMATCH',   r'.'),
        ]
        self.token_specification = specs
        pattern = '|'.join(f"(?P<{name}>{pat})" for name, pat in specs)
        # compiled once per instance
        self._token_regex = re.compile(pattern, re.MULTILINE)

        # internal caches
        self._keyword_set = set(self.keywords)
        if self.config.enable_cache:
            # per-instance LRU cache for tokenize_simple
            self._tokenize_simple_cached = lru_cache(maxsize=self.config.cache_size)(self._tokenize_simple_uncached)
        else:
            self._tokenize_simple_cached = None

    # Public convenience API
    def tokenize(self, code: str) -> List[Token]:
        """Return list of tokens using configured emission (with positions if enabled)."""
        return list(self.iter_tokens(code))

    def iter_tokens(self, code: str) -> Iterator[Token]:
        """Streaming token generator (yields tuples or tuples-with-pos)."""
        line_starts = self._compute_line_starts(code) if self.config.emit_positions else ()
        for mo in self._token_regex.finditer(code):
            kind = mo.lastgroup
            raw = mo.group(kind)
            start_idx = mo.start()
            end_idx = mo.end()

            if kind == 'SKIP':
                continue

            if kind in ('COMMENT', 'ML_COMMENT'):
                if self.config.skip_comments:
                    continue
                # else emit comment token

            if kind == 'NEWLINE':
                if self.config.skip_whitespace:
                    continue
                # else emit newline token

            # Normalize IDs that are keywords
            if kind == 'ID' and raw in self._keyword_set:
                kind = 'KEYWORD'

            # Normalize and convert values
            value: Union[str, int, float] = raw
            if kind == 'STRING':
                # safely unescape using ast.literal_eval
                try:
                    value = ast.literal_eval(raw)
                except Exception:
                    # fallback: strip quotes
                    value = raw[1:-1]
            elif kind == 'NUMBER':
                value = self._normalize_number(raw)
            # MACRO, ID, OP, etc. kept as raw strings

            lineno: Optional[int] = None
            col: Optional[int] = None
            if self.config.emit_positions:
                lineno, col = self._pos_from_index(line_starts, start_idx)

            token = TokenObj(kind, value, lineno, col, start_idx, end_idx)
            yield token.as_tuple(self.config.emit_positions)

    # Backwards-compatible simple tokenize (type,value), with optional caching
    def tokenize_simple(self, code: str) -> List[TokenSimple]:
        if self.config.enable_cache and self._tokenize_simple_cached is not None:
            return list(self._tokenize_simple_cached(code))
        return list(self._tokenize_simple_uncached(code))

    def _tokenize_simple_uncached(self, code: str) -> Iterator[TokenSimple]:
        # produce (type, value) tuples regardless of emit_positions
        for t in self.iter_tokens_simple(code):
            yield t

    def iter_tokens_simple(self, code: str) -> Iterator[TokenSimple]:
        """Generator yielding (type, value) tuples (fast path)"""
        for mo in self._token_regex.finditer(code):
            kind = mo.lastgroup
            raw = mo.group(kind)
            if kind in ('SKIP',):
                continue
            if kind in ('COMMENT', 'ML_COMMENT') and self.config.skip_comments:
                continue
            if kind == 'NEWLINE' and self.config.skip_whitespace:
                continue
            if kind == 'ID' and raw in self._keyword_set:
                kind = 'KEYWORD'
            # For numbers and strings, return normalized values (string/unescaped/number) to aid older consumers
            if kind == 'STRING':
                try:
                    value = ast.literal_eval(raw)
                except Exception:
                    value = raw[1:-1]
            elif kind == 'NUMBER':
                value = self._normalize_number(raw)
            else:
                value = raw
            yield (kind, value)  # type: ignore

    # Utility: return a TokenStream wrapper for parser convenience
    def stream(self, code: str) -> "TokenStream":
        return TokenStream(self.iter_tokens(code), emit_pos=self.config.emit_positions)

    # Helpers ----------------------------------------------------------------
    @staticmethod
    def _normalize_number(raw: str) -> Union[int, float, str]:
        r = raw.replace('_', '')
        try:
            if r.startswith(('0x', '0X')):
                return int(r, 16)
            if r.startswith(('0b', '0B')):
                return int(r, 2)
            if '.' in r:
                return float(r)
            return int(r)
        except Exception:
            return raw  # return raw if parsing fails

    @staticmethod
    def _compute_line_starts(code: str) -> List[int]:
        starts: List[int] = [0]
        for m in re.finditer(r'\n', code):
            starts.append(m.end())
        return starts

    @staticmethod
    def _pos_from_index(line_starts: List[int], index: int) -> Tuple[int, int]:
        # lineno 1-based, col 0-based
        i = bisect.bisect_right(line_starts, index) - 1
        lineno = i + 1
        col = index - line_starts[i]
        return lineno, col


class TokenStream:
    """Simple wrapper around token iterable to provide peek/next and convenience for parsers."""

    def __init__(self, token_iter: Iterable[Token], emit_pos: bool = False):
        self._it = iter(token_iter)
        self._buffer: List[Token] = []
        self.emit_pos = emit_pos

    def _fill(self, n: int = 1) -> None:
        try:
            while len(self._buffer) < n:
                self._buffer.append(next(self._it))
        except StopIteration:
            return

    def peek(self, n: int = 0) -> Optional[Token]:
        self._fill(n + 1)
        return self._buffer[n] if n < len(self._buffer) else None

    def next(self) -> Optional[Token]:
        self._fill(1)
        return self._buffer.pop(0) if self._buffer else None

    def __iter__(self):
        while True:
            t = self.next()
            if t is None:
                break
            yield t


# -------------------------
# CLI self-test (executable)
# -------------------------
if __name__ == "__main__":
    SAMPLE = r'''
-- Load user data
@inject db.conn;

func load_user(uid) {
    quarantine try {
        data = db.get(uid);
        print: "User loaded";
    } replace {
        log("Retrying...");
        load_user(uid);
    } erase {
        alert("Load failed");
    };
};

main() {
    load_user(42);
};
'''

    # Legacy simple usage
    lexer = InstryxLexer(LexerConfig(emit_positions=False))
    simple = lexer.tokenize_simple(SAMPLE)
    print("Simple tokens (first 20):")
    for t in simple[:20]:
        print(t)

    # Rich tokens with positions
    lexer_pos = InstryxLexer(LexerConfig(emit_positions=True))
    tokens_pos = lexer_pos.tokenize(SAMPLE)
    print("\nTokens with positions (first 24):")
    for t in tokens_pos[:24]:
        print(t)

    # TokenStream usage (parser-friendly)
    stream = lexer_pos.stream(SAMPLE)
    print("\nStream peek/next demo:")
    print("peek0:", stream.peek(0))
    print("next:", stream.next())
    print("peek0:", stream.peek(0))

    # Basic self-checks
    assert any(tok[0] == 'KEYWORD' and tok[1] == 'func' for tok in simple), "keyword detection failed"
    print("\ninstryx_lexer self-test passed.")

# instryx_parser.py
# Production-ready Recursive Descent Parser for the Instryx Language
# Author: Violet Magenta / VACU Technologies
# License: MIT

from instryx_lexer import InstryxLexer, Token

class ASTNode:
    """Enhanced AST Node with better performance and debugging capabilities."""
    def __init__(self, node_type: str, value=None, children=None):
        self.node_type = node_type
        self.value = value
        self.children = children if children else []
        
    def __repr__(self):
        return f"ASTNode({self.node_type!r}, {self.value!r}, {self.children!r})"
    
    def __str__(self):
        """Human-readable string representation."""
        if not self.children:
            return f"{self.node_type}({self.value})" if self.value is not None else self.node_type
        return f"{self.node_type}({self.value}, {len(self.children)} children)"


class InstryxParser:
    """
    Optimized recursive descent parser for Instryx with enhanced error handling.
    
    Key improvements:
    - Better error messages with location information
    - Operator precedence parsing
    - Enhanced recovery mechanisms
    - Performance metrics tracking
    """
    
    # Operator precedence (higher number = higher precedence)
    PRECEDENCE = {
        'or': 1, 'and': 2, 'not': 3,
        '==': 4, '!=': 4, '<': 4, '>': 4, '<=': 4, '>=': 4,
        '+': 5, '-': 5,
        '*': 6, '/': 6, '%': 6,
        'unary': 8,
    }
    
    def __init__(self):
        self.lexer = InstryxLexer()
        self.tokens = []
        self.pos = 0
        self._errors = []
        self._stats = {'nodes_created': 0, 'errors_recovered': 0}

    def parse(self, code: str) -> ASTNode:
        """Parse code into AST with comprehensive error handling."""
        try:
            self.tokens = self.lexer.tokenize(code)
            self.pos = 0
            self._errors.clear()
            
            ast = self.program()
            
            # Report any recovered errors
            if self._errors:
                print(f"Parser recovered from {len(self._errors)} errors:")
                for error in self._errors[:5]:  # Show first 5 errors
                    print(f"  - {error}")
            
            return ast
            
        except Exception as e:
            raise SyntaxError(f"Parse failed: {e}") from e

    def current(self) -> Token:
        """Get current token with bounds checking."""
        return self.tokens[self.pos] if self.pos < len(self.tokens) else ('EOF', '')

    def consume(self, expected_type=None) -> Token:
        """Consume token with enhanced error reporting."""
        token = self.current()
        if expected_type and token[0] != expected_type:
            error_msg = f"Expected {expected_type}, got {token[0]} '{token[1]}' at position {self.pos}"
            self._errors.append(error_msg)
            
            # Try error recovery - skip to next expected token type
            if self._try_recovery(expected_type):
                token = self.current()
                self._stats['errors_recovered'] += 1
            else:
                raise SyntaxError(error_msg)
        
        self.pos += 1
        return token
    
    def _try_recovery(self, expected_type: str) -> bool:
        """Attempt to recover from parse error by finding expected token."""
        saved_pos = self.pos
        
        # Look ahead up to 10 tokens for recovery
        for i in range(10):
            if self.pos + i >= len(self.tokens):
                break
            if self.tokens[self.pos + i][0] == expected_type:
                self.pos += i
                return True
        
        # If no recovery found, restore position
        self.pos = saved_pos
        return False

    def match(self, *types) -> bool:
        """Check if current token matches any of the given types."""
        return self.current()[0] in types
    
    def peek(self, offset: int = 1) -> Token:
        """Peek at future token."""
        peek_pos = self.pos + offset
        return self.tokens[peek_pos] if peek_pos < len(self.tokens) else ('EOF', '')

    def program(self) -> ASTNode:
        """Parse top-level program."""
        statements = []
        while not self.match('EOF'):
            try:
                stmt = self.statement()
                if stmt:
                    statements.append(stmt)
            except SyntaxError as e:
                self._errors.append(str(e))
                # Skip to next statement boundary
                self._skip_to_statement_boundary()
                
        node = ASTNode('Program', children=statements)
        self._stats['nodes_created'] += 1
        return node
    
    def _skip_to_statement_boundary(self):
        """Skip tokens until we find a statement boundary."""
        while not self.match('EOF', 'END', 'KEYWORD', 'MACRO', 'RBRACE'):
            self.pos += 1

    def statement(self) -> ASTNode:
        """Parse a statement with enhanced handling."""
        if self.match('KEYWORD'):
            keyword = self.current()[1]
            if keyword == 'func':
                return self.function_definition()
            elif keyword == 'quarantine':
                return self.quarantine_block()
            elif keyword == 'main':
                return self.main_block()
            elif keyword in ('if', 'while', 'for'):
                return self.control_structure()
            elif keyword == 'return':
                return self.return_statement()
        elif self.match('MACRO'):
            return self.macro_statement()
        elif self.match('LBRACE'):
            return self.block()
        else:
            return self.expression_statement()

    def macro_statement(self) -> ASTNode:
        """Parse macro statement with better structure."""
        macro = self.consume('MACRO')
        
        # Handle dotted macro calls like @inject db.conn
        parts = []
        if self.match('ID'):
            parts.append(self.consume('ID')[1])
            
            while self.match('DOT'):
                self.consume('DOT')
                if self.match('ID'):
                    parts.append(self.consume('ID')[1])
        
        self.consume('END')
        
        target = '.'.join(parts) if parts else None
        node = ASTNode('Macro', macro[1], [ASTNode('ID', target)] if target else [])
        self._stats['nodes_created'] += 1
        return node

    def main_block(self) -> ASTNode:
        self.consume('KEYWORD')  # main
        self.consume('LPAREN')
        self.consume('RPAREN')
        body = self.block()
        return ASTNode('Main', children=[body])

    def function_definition(self) -> ASTNode:
        self.consume('KEYWORD')  # func
        name = self.consume('ID')
        self.consume('LPAREN')
        params = []
        if not self.match('RPAREN'):
            params.append(self.consume('ID')[1])
            while self.match('COMMA'):
                self.consume('COMMA')
                params.append(self.consume('ID')[1])
        self.consume('RPAREN')
        body = self.block()
        return ASTNode('Function', name[1], [ASTNode('Params', children=[ASTNode('ID', p) for p in params]), body])

    def quarantine_block(self) -> ASTNode:
        self.consume('KEYWORD')  # quarantine
        try_block = None
        replace_block = None
        erase_block = None
        if self.match('KEYWORD') and self.current()[1] == 'try':
            self.consume('KEYWORD')
            try_block = self.block()
        if self.match('KEYWORD') and self.current()[1] == 'replace':
            self.consume('KEYWORD')
            replace_block = self.block()
        if self.match('KEYWORD') and self.current()[1] == 'erase':
            self.consume('KEYWORD')
            erase_block = self.block()
        return ASTNode('Quarantine', children=[try_block, replace_block, erase_block])

    def block(self) -> ASTNode:
        self.consume('LBRACE')
        stmts = []
        while not self.match('RBRACE'):
            stmts.append(self.statement())
        self.consume('RBRACE')
        return ASTNode('Block', children=stmts)

    def expression_statement(self) -> ASTNode:
        expr = self.expression()
        self.consume('END')
        return ASTNode('ExprStmt', children=[expr])

    def expression(self) -> ASTNode:
        """Parse expression with enhanced operator precedence."""
        return self._parse_assignment()
    
    def _parse_assignment(self) -> ASTNode:
        """Parse assignment expressions."""
        expr = self._parse_or()
        
        if self.match('ASSIGN'):
            if expr.node_type != 'ID':
                raise SyntaxError("Invalid assignment target")
            self.consume('ASSIGN')
            value = self._parse_assignment()  # Right associative
            return ASTNode('Assign', expr.value, [value])
        
        return expr
    
    def _parse_or(self) -> ASTNode:
        """Parse OR expressions."""
        left = self._parse_and()
        
        while self.match('OP') and self.current()[1] == 'or':
            op = self.consume('OP')[1]
            right = self._parse_and()
            left = ASTNode('BinOp', op, [left, right])
            self._stats['nodes_created'] += 1
        
        return left
    
    def _parse_and(self) -> ASTNode:
        """Parse AND expressions."""
        left = self._parse_equality()
        
        while self.match('OP') and self.current()[1] == 'and':
            op = self.consume('OP')[1]
            right = self._parse_equality()
            left = ASTNode('BinOp', op, [left, right])
            self._stats['nodes_created'] += 1
        
        return left
    
    def _parse_equality(self) -> ASTNode:
        """Parse equality expressions."""
        left = self._parse_comparison()
        
        while self.match('OP') and self.current()[1] in ('==', '!='):
            op = self.consume('OP')[1]
            right = self._parse_comparison()
            left = ASTNode('BinOp', op, [left, right])
            self._stats['nodes_created'] += 1
        
        return left
    
    def _parse_comparison(self) -> ASTNode:
        """Parse comparison expressions."""
        left = self._parse_term()
        
        while self.match('OP') and self.current()[1] in ('<', '>', '<=', '>='):
            op = self.consume('OP')[1]
            right = self._parse_term()
            left = ASTNode('BinOp', op, [left, right])
            self._stats['nodes_created'] += 1
        
        return left
    
    def _parse_term(self) -> ASTNode:
        """Parse addition/subtraction."""
        left = self._parse_factor()
        
        while self.match('OP') and self.current()[1] in ('+', '-'):
            op = self.consume('OP')[1]
            right = self._parse_factor()
            left = ASTNode('BinOp', op, [left, right])
            self._stats['nodes_created'] += 1
        
        return left
    
    def _parse_factor(self) -> ASTNode:
        """Parse multiplication/division."""
        left = self._parse_unary()
        
        while self.match('OP') and self.current()[1] in ('*', '/', '%'):
            op = self.consume('OP')[1]
            right = self._parse_unary()
            left = ASTNode('BinOp', op, [left, right])
            self._stats['nodes_created'] += 1
        
        return left
    
    def _parse_unary(self) -> ASTNode:
        """Parse unary expressions."""
        if self.match('OP') and self.current()[1] in ('-', 'not', '!'):
            op = self.consume('OP')[1]
            expr = self._parse_unary()
            self._stats['nodes_created'] += 1
            return ASTNode('UnaryOp', op, [expr])
        
        return self._parse_primary()
    
    def _parse_primary(self) -> ASTNode:
        """Parse primary expressions (literals, identifiers, function calls)."""
        if self.match('ID'):
            id_token = self.consume('ID')
            
            # Check for function call
            if self.match('LPAREN'):
                self.consume('LPAREN')
                args = []
                if not self.match('RPAREN'):
                    args.append(self.expression())
                    while self.match('COMMA'):
                        self.consume('COMMA')
                        args.append(self.expression())
                self.consume('RPAREN')
                self._stats['nodes_created'] += 1
                return ASTNode('Call', id_token[1], args)
            else:
                self._stats['nodes_created'] += 1
                return ASTNode('ID', id_token[1])
                
        elif self.match('STRING'):
            token = self.consume('STRING')
            # Remove quotes and handle basic escapes
            value = token[1][1:-1].replace('\\"', '"')
            self._stats['nodes_created'] += 1
            return ASTNode('String', value)
            
        elif self.match('NUMBER'):
            token = self.consume('NUMBER')
            # Try to convert to appropriate numeric type
            try:
                value = int(token[1]) if '.' not in token[1] else float(token[1])
            except ValueError:
                value = token[1]
            self._stats['nodes_created'] += 1
            return ASTNode('Number', value)
            
        elif self.match('LPAREN'):
            self.consume('LPAREN')
            expr = self.expression()
            self.consume('RPAREN')
            return expr
            
        else:
            raise SyntaxError(f"Unexpected token: {self.current()}")
    
    def control_structure(self) -> ASTNode:
        """Parse control structures (if, while, for)."""
        keyword = self.consume('KEYWORD')[1]
        
        if keyword == 'if':
            condition = self.expression()
            then_block = self.block()
            else_block = None
            
            if self.match('KEYWORD') and self.current()[1] == 'else':
                self.consume('KEYWORD')
                else_block = self.block()
            
            children = [condition, then_block]
            if else_block:
                children.append(else_block)
            self._stats['nodes_created'] += 1
            return ASTNode('If', children=children)
            
        elif keyword == 'while':
            condition = self.expression()
            body = self.block()
            self._stats['nodes_created'] += 1
            return ASTNode('While', children=[condition, body])
            
        else:
            raise SyntaxError(f"Unsupported control structure: {keyword}")
    
    def return_statement(self) -> ASTNode:
        """Parse return statement."""
        self.consume('KEYWORD')  # 'return'
        
        expr = None
        if not self.match('END'):
            expr = self.expression()
        
        if self.match('END'):
            self.consume('END')
        
        children = [expr] if expr else []
        self._stats['nodes_created'] += 1
        return ASTNode('Return', children=children)
    
    def get_stats(self) -> dict:
        """Get parser performance statistics."""
        return {
            **self._stats,
            'errors': len(self._errors),
            'tokens_consumed': self.pos
        }


# Test block (can be removed in production)
if __name__ == "__main__":
    parser = InstryxParser()
    sample_code = """
    -- Load user data
    @inject db.conn;

    func load_user(uid) {
        quarantine try {
            data = db.get(uid);
            print: "User loaded";
        } replace {
            log("Retrying...");
            load_user(uid);
        } erase {
            alert("Load failed");
        };
    };

    main() {
        load_user(42);
    };
    """
    ast = parser.parse(sample_code)
    print(ast)

# instryx_parser.py
# Production-ready Recursive Descent Parser for the Instryx Language â€” supreme boosters
# Author: Violet Magenta / VACU Technologies (modified)
# License: MIT
"""
Enhanced Instryx recursive-descent parser.

Additions / improvements:
 - Richer ASTNode with optional source location (lineno, col)
 - Robust error messages with source context
 - Expression parsing with operator precedence and constant folding hints
 - Better recovery: skip to safe token boundaries on syntax error
 - Support for common statements: func, main, quarantine, macro, import, return, if, while, assignments, calls, literals
 - Utilities: peek(n), match, consume with safe checks
 - CLI test harness at module bottom
Notes:
 - Keeps compatibility with earlier ASTNode shape (node_type, value, children)
 - Uses InstryxLexer.tokenize(code) â€” token shape is flexible (tuple-like); parser adapts to 2- or 3-tuple tokens
"""

from dataclasses import dataclass
from typing import List, Optional, Tuple, Any

from instryx_lexer import InstryxLexer, Token

# Minimal expectation for tokens: token is tuple-like (type, value, ...) where type and value accessible via indexes.
# Parser will adapt if extra positional elements exist (e.g., location).


@dataclass
class ASTNode:
    node_type: str
    value: Optional[Any] = None
    children: List["ASTNode"] = None
    lineno: Optional[int] = None
    col: Optional[int] = None

    def __post_init__(self):
        if self.children is None:
            self.children = []

    def __repr__(self):
        return f"ASTNode({self.node_type!r}, {self.value!r}, {self.children!r})"


class ParseError(SyntaxError):
    pass


    def main_block(self) -> ASTNode:
        self.consume('KEYWORD')  # main
        self.consume('LPAREN')
        self.consume('RPAREN')
        body = self.block()
        return ASTNode('Main', children=[body])

    def function_definition(self) -> ASTNode:
        self.consume('KEYWORD')  # func
        name_tok = self.consume('ID')
        name = name_tok[1]
        self.consume('LPAREN')
        params = []
        if not self.match('RPAREN'):
            # accept comma-separated ids
            params.append(self.consume('ID')[1])
            while self.match('COMMA'):
                self.consume('COMMA')
                params.append(self.consume('ID')[1])
        self.consume('RPAREN')
        body = self.block()
        params_node = ASTNode('Params', children=[ASTNode('ID', p) for p in params])
        return ASTNode('Function', name, [params_node, body])

    def quarantine_block(self) -> ASTNode:
        self.consume('KEYWORD')  # quarantine
        try_block = None
        replace_block = None
        erase_block = None
        if self.match('KEYWORD') and self._token_value(self.current()) == 'try':
            self.consume('KEYWORD')
            try_block = self.block()
        if self.match('KEYWORD') and self._token_value(self.current()) == 'replace':
            self.consume('KEYWORD')
            replace_block = self.block()
        if self.match('KEYWORD') and self._token_value(self.current()) == 'erase':
            self.consume('KEYWORD')
            erase_block = self.block()
        return ASTNode('Quarantine', children=[try_block, replace_block, erase_block])

    def block(self) -> ASTNode:
        self.consume('LBRACE')
        stmts: List[ASTNode] = []
        while not self.match('RBRACE'):
            if self.match('EOF'):
                self._syntax_error("Unexpected EOF while parsing block", self.current())
            stmts.append(self.statement())
        self.consume('RBRACE')
        return ASTNode('Block', children=stmts)

    def return_statement(self) -> ASTNode:
        self.consume('KEYWORD')  # return
        # optional expression
        if self.match('END'):
            self.consume('END')
            return ASTNode('Return')
        expr = self.expression()
        self.consume('END')
        return ASTNode('Return', children=[expr])

    def if_statement(self) -> ASTNode:
        self.consume('KEYWORD')  # if
        cond = self.expression()
        then_block = self.block()
        else_block = None
        if self.match('KEYWORD') and self._token_value(self.current()) == 'else':
            self.consume('KEYWORD')
            if self.match('KEYWORD') and self._token_value(self.current()) == 'if':
                else_block = self.if_statement()
            else:
                else_block = self.block()
        return ASTNode('If', children=[cond, then_block, else_block])

    def while_statement(self) -> ASTNode:
        self.consume('KEYWORD')  # while
        cond = self.expression()
        body = self.block()
        return ASTNode('While', children=[cond, body])

    def expression_statement(self) -> ASTNode:
        expr = self.expression()
        # optional END token (some sources may omit; be permissive)
        if self.match('END'):
            self.consume('END')
        return ASTNode('ExprStmt', children=[expr])

    # -------------------------
    # Expression parsing (precedence climbing)
    # -------------------------
    # Define operator precedence (higher number -> higher precedence)
    _PREC: Dict[str, int] = {
        '||': 1,
        '&&': 2,
        '==': 3, '!=': 3,
        '<': 4, '>': 4, '<=': 4, '>=': 4,
        '+': 5, '-': 5,
        '*': 6, '/': 6, '%': 6,
    }

    def expression(self, min_prec: int = 1) -> ASTNode:
        # parse left-hand side
        left = self._parse_unary()
        while True:
            tok = self.current()
            ttype = self._token_type(tok)
            tval = self._token_value(tok)
            # determine operator text
            op = None
            if ttype == 'OP':
                op = tval
            elif ttype in ('PLUS', 'MINUS', 'STAR', 'SLASH', 'PERCENT', 'EQ', 'NEQ', 'LT', 'GT', 'LE', 'GE', 'AND', 'OR'):
                op = tval
            elif ttype in ('||', '&&', '==', '!=', '<', '>', '<=', '>=', '+', '-', '*', '/', '%'):
                op = ttype
            if not op or op not in self._PREC:
                break
            prec = self._PREC[op]
            if prec < min_prec:
                break
            # consume operator
            self.consume(ttype)
            # parse right-hand side with higher precedence for right-associative operators if any (none here)
            rhs = self.expression(prec + 1)
            left = ASTNode('Binary', op, [left, rhs])
        return left

    def _parse_unary(self) -> ASTNode:
        tok = self.current()
        ttype = self._token_type(tok)
        tval = self._token_value(tok)
        # unary operators
        if ttype == 'OP' and tval in ('+', '-', '!'):
            self.consume('OP')
            operand = self._parse_unary()
            return ASTNode('Unary', tval, [operand])
        # primary
        return self._parse_primary()

    def _parse_primary(self) -> ASTNode:
        tok = self.current()
        ttype = self._token_type(tok)
        if ttype == 'ID':
            idtok = self.consume('ID')
            # assignment: id = expr
            if self.match('ASSIGN'):
                self.consume('ASSIGN')
                expr = self.expression()
                return ASTNode('Assign', idtok[1], [expr])
            # function call: id(...)
            if self.match('LPAREN'):
                self.consume('LPAREN')
                args = []
                if not self.match('RPAREN'):
                    args.append(self.expression())
                    while self.match('COMMA'):
                        self.consume('COMMA')
                        args.append(self.expression())
                self.consume('RPAREN')
                return ASTNode('Call', idtok[1], args)
            # directive style: id : expr ;  (common e.g., print: "x";)
            if self.match('COLON'):
                self.consume('COLON')
                expr = self.expression()
                # optional END
                if self.match('END'):
                    self.consume('END')
                return ASTNode('Directive', idtok[1], [expr])
            return ASTNode('ID', idtok[1])
        if ttype == 'STRING':
            val = self.consume('STRING')[1]
            return ASTNode('String', val)
        if ttype == 'NUMBER':
            val = self.consume('NUMBER')[1]
            return ASTNode('Number', val)
        if ttype == 'LPAREN':
            self.consume('LPAREN')
            expr = self.expression()
            self.consume('RPAREN')
            return expr
        # unexpected
        self._syntax_error("Unexpected token in expression", tok)

# -------------------------
# Test harness
# -------------------------
if __name__ == "__main__":
    parser = InstryxParser()
    sample_code = """
    -- Load user data
    @inject db.conn;

    func load_user(uid) {
        quarantine try {
            data = db.get(uid);
            print: "User loaded";
        } replace {
            log("Retrying...");
            load_user(uid);
        } erase {
            alert("Load failed");
        };
    };

    main() {
        load_user(42);
    };
    """
    ast = parser.parse(sample_code)
    print(ast)

"""
instryx_parser.py

Enhanced Recursive Descent Parser for the Instryx language â€” supreme boosters.

Additions & improvements (fully implemented):
 - Rich ASTNode dataclass with source location (lineno, col) and span (offsets)
 - Robust token helpers that tolerate different token tuple shapes (2- or 4-tuple)
 - Clear, contextual ParseError messages and diagnostics collection
 - Operator-precedence (precedence-climbing) expression parsing with unary operators
 - Support for statements: func, main, quarantine, macro, import, return, if, while, block
 - Expression forms: Assign, Call, Directive (id: expr; normalized to Call), Var, Number, String, Binary, Unary
 - Recoverable parsing using _recover_to to skip to safe boundaries on error
 - Utilities: pretty_print, ast_to_dict, find_nodes
 - Optional cached parse entrypoint `parse_cached(code)` using LRU cache
 - CLI test harness that prints AST and diagnostics
 - Safe to call multiple times (no hidden global state leaks)

Requires: instryx_lexer.InstryxLexer.tokenize that yields tokens in one of:
  ('TYPE', 'value') or ('TYPE', 'value', lineno, col) or ('TYPE', 'value', (lineno, col))

Author: Violet Magenta / VACU Technologies (modified)
License: MIT
"""

from dataclasses import dataclass, field, asdict
from typing import List, Optional, Tuple, Any, Dict, Iterable, Callable
from functools import lru_cache

try:
    from instryx_lexer import InstryxLexer, Token
except Exception:  # pragma: no cover - lexer must exist in real environment
    # Minimal fallback for tests: tiny lexer
    Token = Tuple[str, str]
    class InstryxLexer:
        def tokenize(self, code: str):
            # Extremely small fallback: split on whitespace and punctuation for demo only.
            # Real project must provide proper lexer.
            for part in code.replace("(", " ( ").replace(")", " ) ").replace("{", " { ").replace("}", " } ").split():
                if part.isnumeric():
                    yield ('NUMBER', part)
                elif part.startswith('"') and part.endswith('"'):
                    yield ('STRING', part)
                elif part in ('func','main','quarantine','try','replace','erase','if','else','while','return','import'):
                    yield ('KEYWORD', part)
                elif part == ';':
                    yield ('END', ';')
                elif part == ',':
                    yield ('COMMA', ',')
                elif part == '{':
                    yield ('LBRACE', '{')
                elif part == '}':
                    yield ('RBRACE', '}')
                elif part == '(':
                    yield ('LPAREN', '(')
                elif part == ')':
                    yield ('RPAREN', ')')
                else:
                    yield ('ID', part)


# -------------------------
# AST Node
# -------------------------
@dataclass
class ASTNode:
    node_type: str
    value: Optional[Any] = None
    children: List["ASTNode"] = field(default_factory=list)
    lineno: Optional[int] = None
    col: Optional[int] = None
    span: Optional[Tuple[int, int]] = None

    def __repr__(self) -> str:
        return f"ASTNode({self.node_type!r}, {self.value!r}, {self.children!r})"

    def to_dict(self) -> Dict[str, Any]:
        return {
            "node_type": self.node_type,
            "value": self.value,
            "children": [c.to_dict() for c in self.children],
            "lineno": self.lineno,
            "col": self.col,
            "span": self.span,
        }


class ParseError(SyntaxError):
    pass


# -------------------------
# Parser
# -------------------------
